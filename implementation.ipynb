{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: A Method for Stochastic Optimization\n",
    "\n",
    "This notebook implements the Adam optimizer from scratch based on the paper by Kingma and Ba (2014). We will compare its convergence properties against standard Stochastic Gradient Descent (SGD) on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Adam Implementation\n",
    "\n",
    "Here we implement the optimizer class inheriting from `torch.optim.Optimizer`. This explicitly codes Equation 1 through Equation 8 from the Adam paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamFromScratch(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "            \n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(AdamFromScratch, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients')\n",
    "\n",
    "                # L2 Regularization (Weight Decay)\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(p.data, alpha=group['weight_decay'])\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t = beta1 * m_{t-1} + (1 - beta1) * g_t\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                \n",
    "                # v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Bias correction\n",
    "                # m_hat = m_t / (1 - beta1^t)\n",
    "                bias_correction1 = 1 - beta1 ** t\n",
    "                m_hat = exp_avg / bias_correction1\n",
    "                \n",
    "                # v_hat = v_t / (1 - beta2^t)\n",
    "                bias_correction2 = 1 - beta2 ** t\n",
    "                v_hat = exp_avg_sq / bias_correction2\n",
    "\n",
    "                # Update parameters\n",
    "                # theta = theta - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                denom = v_hat.sqrt().add_(group['eps'])\n",
    "                step_size = group['lr']\n",
    "                \n",
    "                p.data.addcdiv_(m_hat, denom, value=-step_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and Model Setup\n",
    "\n",
    "We use MNIST (Modified National Institute of Standards and Technology), a database of handwritten digits. It serves as a standard benchmark for optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                         download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Simple Multi-Layer Perceptron\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop and Experiment\n",
    "\n",
    "We define a generic training function and run two experiments:\n",
    "1. Training with our **Custom Adam**.\n",
    "2. Training with **Standard SGD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(optimizer_class, learning_rate, train_loader, epochs=3, name=\"Adam\"):\n",
    "    model = SimpleMLP().to(device)\n",
    "    \n",
    "    # Instantiate optimizer\n",
    "    if name == \"AdamFromScratch\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        # For comparison, we use PyTorch built-in SGD\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"Starting training with {name}...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log loss every 10 steps for smoother plotting\n",
    "            if batch_idx % 10 == 0:\n",
    "                losses.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "        \n",
    "    return losses\n",
    "\n",
    "# Run Experiments\n",
    "print(\"Running Adam Experiment...\")\n",
    "adam_losses = train_model(AdamFromScratch, lr=0.001, train_loader=train_loader, epochs=2, name=\"AdamFromScratch\")\n",
    "\n",
    "print(\"Running SGD Experiment...\")\n",
    "sgd_losses = train_model(torch.optim.SGD, lr=0.01, train_loader=train_loader, epochs=2, name=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization of Convergence\n",
    "\n",
    "We compare the training loss curves. Adam typically shows faster convergence in the initial phases of training compared to standard SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting\n",
    "plt.plot(adam_losses, label='Adam (Custom Impl.)', color='royalblue', alpha=0.9, linewidth=1.5)\n",
    "plt.plot(sgd_losses, label='SGD (Standard)', color='darkorange', alpha=0.9, linewidth=1.5)\n",
    "\n",
    "plt.title('Convergence Analysis: Adam vs SGD on MNIST', fontsize=16)\n",
    "plt.xlabel('Training Iterations (x10)', fontsize=12)\n",
    "plt.ylabel('Cross Entropy Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}